{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:13:37.387347Z",
     "start_time": "2018-01-16T21:13:35.028943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import kapre\n",
    "import arrow\n",
    "import pprint\n",
    "import threading\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.signal\n",
    "from soph import center_wave\n",
    "from soph import ex_generator as old_gen\n",
    "\n",
    "ex_df = pd.read_pickle(\"data/ex_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:13:38.430471Z",
     "start_time": "2018-01-16T21:13:38.425229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:13:42.054808Z",
     "start_time": "2018-01-16T21:13:39.179159Z"
    }
   },
   "outputs": [],
   "source": [
    "VAL = [\"val\", \"test\"]\n",
    "TRAIN = [\"train\"]\n",
    "N_BATCH = 512\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=6, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=.5, patience=3, verbose=1, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "\n",
    "val_data = next(old_gen(\n",
    "    batch_size=sum(ex_df.state.isin(VAL)),\n",
    "    shuffle=False,\n",
    "    raw_label=True,\n",
    "    state=VAL))\n",
    "\n",
    "traing_gen = old_gen(\n",
    "    batch_size=N_BATCH,\n",
    "    raw_label=True,\n",
    "    state=TRAIN,\n",
    "    vol_range=.1,\n",
    "    p_transform=1,\n",
    "    shift=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "100*(1/32)": {},
     "100*(2380/64727)": {}
    }
   },
   "source": [
    "# Test base RNN\n",
    "\n",
    "There are 32 classes. \n",
    "\n",
    "Random guesses (balanced) give {{100*(1/32)}}% accuracy. \n",
    "\n",
    "Guessing the largest class (unbalanced) give {{100*(2380/64727)}}% accuracy.\n",
    "\n",
    "First, as an experiment, let's look at a 1D convolution on raw audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T22:00:49.362656Z",
     "start_time": "2018-01-15T22:00:49.360391Z"
    }
   },
   "source": [
    "# Base Spectrogram RNN\n",
    "\n",
    "A spectrogram is constructed through a short-time fourier transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T16:05:25.648408Z",
     "start_time": "2018-01-16T16:05:25.584008Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogMagSpectrogram(keras.engine.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 frame_length=1024,\n",
    "                 frame_step=512,\n",
    "                 fft_length=1024,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.fft_length = fft_length\n",
    "        self.log_offset = 1e-6\n",
    "        super(LogMagSpectrogram, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(LogMagSpectrogram, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch = input_shape[0]\n",
    "        n_samples = input_shape[1]\n",
    "        n_seq = n_samples//self.frame_step\n",
    "        n_bins = self.fft_length//2 +1\n",
    "        \n",
    "        return batch, n_seq, n_bins\n",
    "\n",
    "    def call(self, x):\n",
    "        # `stfts` is a complex64 Tensor representing the Short-time Fourier Transform of\n",
    "        # each signal in `signals`. Its shape is [batch_size, ?, fft_unique_bins]\n",
    "        # where fft_unique_bins = fft_length // 2 + 1 = 513.\n",
    "        stfts = tf.contrib.signal.stft(\n",
    "            x,\n",
    "            frame_length=self.frame_length,\n",
    "            frame_step=self.frame_step,\n",
    "            fft_length=self.fft_length,\n",
    "            pad_end=True,\n",
    "        )\n",
    "\n",
    "        # An energy spectrogram is the magnitude of the complex-valued STFT.\n",
    "        # A float32 Tensor of shape [batch_size, ?, 513].\n",
    "        magnitude_spectrograms = tf.abs(stfts)\n",
    "\n",
    "        log_magnitude_spectrograms = tf.log(\n",
    "            magnitude_spectrograms + self.log_offset)\n",
    "        return log_magnitude_spectrograms\n",
    "\n",
    "    def get_config(self):\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.fft_length = fft_length\n",
    "        self.log_offset = 1e-6\n",
    "        config = {\n",
    "            'frame_length': self.frame_length,\n",
    "            'frame_step': self.frame_step,\n",
    "            'fft_length': self.fft_length,\n",
    "            'log_offset': self.log_offset\n",
    "        }\n",
    "        base_config = super(Spectrogram, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:01:17.408383Z",
     "start_time": "2018-01-16T14:01:16.577313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 100, 513]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "log_mag_spectrogram_1 (LogMa (None, 100, 513)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 513)          2052      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 513)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 50)           25700     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 50)           200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               30300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                3232      \n",
      "=================================================================\n",
      "Total params: 61,884\n",
      "Trainable params: 60,558\n",
      "Non-trainable params: 1,326\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SR = 16000\n",
    "N_SEQ = 100\n",
    "N_STEP = SR//(N_SEQ)\n",
    "N_LEN = 2*N_STEP\n",
    "N_DFT = max(2**(int(log2(N_STEP))+1),1024)\n",
    "DROP = .25\n",
    "INIT = \"he_normal\"\n",
    "ACT = \"elu\"\n",
    "N_CAT = int(ex_df.raw_label_i.max()+1)\n",
    "N_BATCH = 512\n",
    "VAL = [\"val\", \"test\"]\n",
    "TRAIN = [\"train\"]\n",
    "REG = None # keras.regularizers.l2(0.1)\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(SR,))\n",
    "input_block = LogMagSpectrogram(frame_length=N_LEN, frame_step=N_STEP, fft_length=1024)(input_layer)\n",
    "\n",
    "input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "input_block = keras.layers.Dense(50, activation=ACT)(input_block)\n",
    "input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "rnn_block = input_block\n",
    "\n",
    "rnn_block = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(\n",
    "        50,\n",
    "        activation=ACT,\n",
    "        kernel_initializer=INIT,\n",
    "        dropout=DROP,\n",
    "        recurrent_dropout=DROP,\n",
    "        kernel_regularizer=REG, \n",
    "        recurrent_regularizer=REG, \n",
    "        bias_regularizer=REG\n",
    "    ), merge_mode=\"concat\")(rnn_block)\n",
    "rnn_block = keras.layers.BatchNormalization()(rnn_block)\n",
    "\n",
    "output_layer = keras.layers.Dense(\n",
    "    N_CAT, activation=\"softmax\", kernel_initializer=INIT)(rnn_block)\n",
    "rnn_model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "rnn_model.summary()\n",
    "rnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:45:47.149327Z",
     "start_time": "2018-01-16T14:01:19.411198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "104/103 [==============================] - 51s 492ms/step - loss: 3.2995 - sparse_categorical_accuracy: 0.1140 - val_loss: 5.0470 - val_sparse_categorical_accuracy: 0.0312\n",
      "Epoch 2/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 2.3705 - sparse_categorical_accuracy: 0.3213 - val_loss: 1.8521 - val_sparse_categorical_accuracy: 0.4044\n",
      "Epoch 3/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 1.4349 - sparse_categorical_accuracy: 0.5745 - val_loss: 1.2609 - val_sparse_categorical_accuracy: 0.5890\n",
      "Epoch 4/200\n",
      "104/103 [==============================] - 45s 437ms/step - loss: 1.2317 - sparse_categorical_accuracy: 0.6405 - val_loss: 1.0480 - val_sparse_categorical_accuracy: 0.6968\n",
      "Epoch 5/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 1.0311 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.9681 - val_sparse_categorical_accuracy: 0.7270\n",
      "Epoch 6/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.9506 - sparse_categorical_accuracy: 0.7226 - val_loss: 0.9334 - val_sparse_categorical_accuracy: 0.7521\n",
      "Epoch 7/200\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.8652 - sparse_categorical_accuracy: 0.7375 - val_loss: 0.8017 - val_sparse_categorical_accuracy: 0.7822\n",
      "Epoch 8/200\n",
      "104/103 [==============================] - 45s 435ms/step - loss: 0.7562 - sparse_categorical_accuracy: 0.7720 - val_loss: 0.8046 - val_sparse_categorical_accuracy: 0.7844\n",
      "Epoch 9/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.7025 - sparse_categorical_accuracy: 0.7866 - val_loss: 0.7585 - val_sparse_categorical_accuracy: 0.8062\n",
      "Epoch 10/200\n",
      "104/103 [==============================] - 46s 441ms/step - loss: 0.6019 - sparse_categorical_accuracy: 0.8131 - val_loss: 0.7232 - val_sparse_categorical_accuracy: 0.8169\n",
      "Epoch 11/200\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.5670 - sparse_categorical_accuracy: 0.8282 - val_loss: 0.6699 - val_sparse_categorical_accuracy: 0.8173\n",
      "Epoch 12/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.5162 - sparse_categorical_accuracy: 0.8425 - val_loss: 0.6508 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 13/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.5212 - sparse_categorical_accuracy: 0.8395 - val_loss: 0.6353 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 14/200\n",
      "104/103 [==============================] - 45s 429ms/step - loss: 0.5025 - sparse_categorical_accuracy: 0.8481 - val_loss: 0.5901 - val_sparse_categorical_accuracy: 0.8492\n",
      "Epoch 15/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.4951 - sparse_categorical_accuracy: 0.8488 - val_loss: 0.6001 - val_sparse_categorical_accuracy: 0.8391\n",
      "Epoch 16/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.4386 - sparse_categorical_accuracy: 0.8672 - val_loss: 0.5591 - val_sparse_categorical_accuracy: 0.8537\n",
      "Epoch 17/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.4552 - sparse_categorical_accuracy: 0.8637 - val_loss: 0.5639 - val_sparse_categorical_accuracy: 0.8561\n",
      "Epoch 18/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.4523 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5666 - val_sparse_categorical_accuracy: 0.8535\n",
      "Epoch 19/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.4035 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.5383 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 20/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.3889 - sparse_categorical_accuracy: 0.8820 - val_loss: 0.5755 - val_sparse_categorical_accuracy: 0.8599\n",
      "Epoch 21/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.4410 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.5176 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 22/200\n",
      "104/103 [==============================] - 45s 437ms/step - loss: 0.3901 - sparse_categorical_accuracy: 0.8817 - val_loss: 0.5090 - val_sparse_categorical_accuracy: 0.8754\n",
      "Epoch 23/200\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.3574 - sparse_categorical_accuracy: 0.8875 - val_loss: 0.5047 - val_sparse_categorical_accuracy: 0.8760\n",
      "Epoch 24/200\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.3488 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.4914 - val_sparse_categorical_accuracy: 0.8788\n",
      "Epoch 25/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3934 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.5284 - val_sparse_categorical_accuracy: 0.8761\n",
      "Epoch 26/200\n",
      "104/103 [==============================] - 46s 438ms/step - loss: 0.4284 - sparse_categorical_accuracy: 0.8694 - val_loss: 0.5723 - val_sparse_categorical_accuracy: 0.8686\n",
      "Epoch 27/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.4037 - sparse_categorical_accuracy: 0.8766 - val_loss: 0.5859 - val_sparse_categorical_accuracy: 0.8699\n",
      "Epoch 28/200\n",
      "103/103 [============================>.] - ETA: 0s - loss: 0.3773 - sparse_categorical_accuracy: 0.8871\n",
      "Epoch 00028: reducing learning rate to 0.0010000000474974513.\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.3773 - sparse_categorical_accuracy: 0.8870 - val_loss: 0.5700 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 29/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3904 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5189 - val_sparse_categorical_accuracy: 0.8842\n",
      "Epoch 30/200\n",
      "104/103 [==============================] - 45s 431ms/step - loss: 0.4019 - sparse_categorical_accuracy: 0.8795 - val_loss: 0.4678 - val_sparse_categorical_accuracy: 0.8910\n",
      "Epoch 31/200\n",
      "104/103 [==============================] - 45s 435ms/step - loss: 0.4103 - sparse_categorical_accuracy: 0.8774 - val_loss: 0.4816 - val_sparse_categorical_accuracy: 0.8879\n",
      "Epoch 32/200\n",
      "104/103 [==============================] - 46s 438ms/step - loss: 0.3743 - sparse_categorical_accuracy: 0.8907 - val_loss: 0.4412 - val_sparse_categorical_accuracy: 0.8939\n",
      "Epoch 33/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3480 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.8949\n",
      "Epoch 34/200\n",
      "104/103 [==============================] - 45s 435ms/step - loss: 0.3313 - sparse_categorical_accuracy: 0.8978 - val_loss: 0.4537 - val_sparse_categorical_accuracy: 0.8978\n",
      "Epoch 35/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3413 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.4185 - val_sparse_categorical_accuracy: 0.8949\n",
      "Epoch 36/200\n",
      "104/103 [==============================] - 45s 431ms/step - loss: 0.3711 - sparse_categorical_accuracy: 0.8850 - val_loss: 0.4750 - val_sparse_categorical_accuracy: 0.8918\n",
      "Epoch 37/200\n",
      "104/103 [==============================] - 45s 428ms/step - loss: 0.3793 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.4613 - val_sparse_categorical_accuracy: 0.8978\n",
      "Epoch 38/200\n",
      "104/103 [==============================] - 45s 431ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.8790 - val_loss: 0.4137 - val_sparse_categorical_accuracy: 0.8937\n",
      "Epoch 39/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.3845 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.4125 - val_sparse_categorical_accuracy: 0.9014\n",
      "Epoch 40/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3702 - sparse_categorical_accuracy: 0.8924 - val_loss: 0.4098 - val_sparse_categorical_accuracy: 0.8990\n",
      "Epoch 41/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3517 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.4474 - val_sparse_categorical_accuracy: 0.8965\n",
      "Epoch 42/200\n",
      "104/103 [==============================] - 45s 431ms/step - loss: 0.3327 - sparse_categorical_accuracy: 0.8963 - val_loss: 0.4199 - val_sparse_categorical_accuracy: 0.8997\n",
      "Epoch 43/200\n",
      "104/103 [==============================] - 46s 438ms/step - loss: 0.3410 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.3857 - val_sparse_categorical_accuracy: 0.9064\n",
      "Epoch 44/200\n",
      "104/103 [==============================] - 46s 438ms/step - loss: 0.3777 - sparse_categorical_accuracy: 0.8869 - val_loss: 0.4022 - val_sparse_categorical_accuracy: 0.9002\n",
      "Epoch 45/200\n",
      "104/103 [==============================] - 45s 437ms/step - loss: 0.3682 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.4099 - val_sparse_categorical_accuracy: 0.9023\n",
      "Epoch 46/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3274 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4020 - val_sparse_categorical_accuracy: 0.9012\n",
      "Epoch 47/200\n",
      "103/103 [============================>.] - ETA: 0s - loss: 0.3048 - sparse_categorical_accuracy: 0.9073\n",
      "Epoch 00047: reducing learning rate to 0.0005000000237487257.\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.3049 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4144 - val_sparse_categorical_accuracy: 0.9031\n",
      "Epoch 48/200\n",
      "104/103 [==============================] - 45s 436ms/step - loss: 0.3319 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.3810 - val_sparse_categorical_accuracy: 0.9084\n",
      "Epoch 49/200\n",
      "104/103 [==============================] - 45s 431ms/step - loss: 0.3489 - sparse_categorical_accuracy: 0.8935 - val_loss: 0.3890 - val_sparse_categorical_accuracy: 0.9110\n",
      "Epoch 50/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3583 - sparse_categorical_accuracy: 0.8936 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.9144\n",
      "Epoch 51/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3554 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.3686 - val_sparse_categorical_accuracy: 0.9124\n",
      "Epoch 52/200\n",
      "104/103 [==============================] - 45s 430ms/step - loss: 0.3439 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.3616 - val_sparse_categorical_accuracy: 0.9128\n",
      "Epoch 53/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3637 - sparse_categorical_accuracy: 0.8924 - val_loss: 0.3355 - val_sparse_categorical_accuracy: 0.9128\n",
      "Epoch 54/200\n",
      "104/103 [==============================] - 45s 433ms/step - loss: 0.3303 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3966 - val_sparse_categorical_accuracy: 0.9068\n",
      "Epoch 55/200\n",
      "104/103 [==============================] - 45s 429ms/step - loss: 0.3636 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.3592 - val_sparse_categorical_accuracy: 0.9117\n",
      "Epoch 56/200\n",
      "104/103 [==============================] - 45s 428ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8893 - val_loss: 0.3709 - val_sparse_categorical_accuracy: 0.9095\n",
      "Epoch 57/200\n",
      "103/103 [============================>.] - ETA: 0s - loss: 0.3399 - sparse_categorical_accuracy: 0.8952\n",
      "Epoch 00057: reducing learning rate to 0.0002500000118743628.\n",
      "104/103 [==============================] - 44s 424ms/step - loss: 0.3395 - sparse_categorical_accuracy: 0.8951 - val_loss: 0.4227 - val_sparse_categorical_accuracy: 0.9014\n",
      "Epoch 58/200\n",
      "104/103 [==============================] - 45s 434ms/step - loss: 0.3907 - sparse_categorical_accuracy: 0.8904 - val_loss: 0.3827 - val_sparse_categorical_accuracy: 0.9101\n",
      "Epoch 59/200\n",
      "104/103 [==============================] - 45s 432ms/step - loss: 0.3962 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.3588 - val_sparse_categorical_accuracy: 0.9133\n",
      "Epoch 00059: early stopping\n"
     ]
    }
   ],
   "source": [
    "traing_gen = old_gen(\n",
    "    batch_size=N_BATCH,\n",
    "    raw_label=True,\n",
    "    state=TRAIN,\n",
    "    vol_range=.1,\n",
    "    p_transform=1,\n",
    "    shift=0\n",
    ")\n",
    "\n",
    "history = rnn_model.fit_generator(\n",
    "    generator=traing_gen,\n",
    "    steps_per_epoch=sum(ex_df.state.isin(TRAIN)) / N_BATCH,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Spectrogram with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Mel Spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:50:42.866664Z",
     "start_time": "2018-01-16T14:50:42.733327Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogMelSpectrogram(keras.engine.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 frame_length=1024,\n",
    "                 frame_step=512,\n",
    "                 fft_length=1024,\n",
    "                 lower_edge_hertz=80.0,\n",
    "                 upper_edge_hertz=7600.0,\n",
    "                 num_mel_bins=64,\n",
    "                 sr=16000,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.fft_length = fft_length\n",
    "        self.n_bins = self.fft_length // 2 + 1\n",
    "        self.log_offset = 1e-6\n",
    "        self.lower_edge_hertz = lower_edge_hertz\n",
    "        self.upper_edge_hertz = upper_edge_hertz\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        self.sr = sr\n",
    "        super(LogMelSpectrogram, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
    "            self.num_mel_bins, self.n_bins, self.sr, self.lower_edge_hertz,\n",
    "            self.upper_edge_hertz)\n",
    "        self.non_trainable_weights.append(self.linear_to_mel_weight_matrix)\n",
    "        \n",
    "        super(LogMelSpectrogram, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch = input_shape[0]\n",
    "        n_samples = input_shape[1]\n",
    "        n_seq = n_samples//self.frame_step\n",
    "\n",
    "        return batch, n_seq, self.num_mel_bins\n",
    "\n",
    "    def call(self, x):\n",
    "        # `stfts` is a complex64 Tensor representing the Short-time Fourier Transform of\n",
    "        # each signal in `signals`. Its shape is [batch_size, ?, fft_unique_bins]\n",
    "        # where fft_unique_bins = fft_length // 2 + 1 = 513.\n",
    "        stfts = tf.contrib.signal.stft(\n",
    "            x,\n",
    "            frame_length=self.frame_length,\n",
    "            frame_step=self.frame_step,\n",
    "            fft_length=self.fft_length,\n",
    "            pad_end=True,\n",
    "        )\n",
    "\n",
    "        # An energy spectrogram is the magnitude of the complex-valued STFT.\n",
    "        # A float32 Tensor of shape [batch_size, ?, 513].\n",
    "        magnitude_spectrograms = tf.abs(stfts)\n",
    "\n",
    "        #         log_magnitude_spectrograms = tf.log(\n",
    "        #             magnitude_spectrograms + self.log_offset)\n",
    "\n",
    "#         mel_spectrograms = tf.tensordot(magnitude_spectrograms,\n",
    "#                                         self.linear_to_mel_weight_matrix, 1)\n",
    "        # Note: Shape inference for `tf.tensordot` does not currently handle this case.\n",
    "#         mel_spectrograms.set_shape(\n",
    "#             magnitude_spectrograms.shape[:-1].concatenate(\n",
    "#                 self.linear_to_mel_weight_matrix.shape[-1:]))\n",
    "        \n",
    "        mel_spectrograms = K.dot(magnitude_spectrograms, self.linear_to_mel_weight_matrix)\n",
    "\n",
    "        log_mel_spectrograms = tf.log(mel_spectrograms + self.log_offset)\n",
    "        \n",
    "        return mel_spectrograms\n",
    "\n",
    "    def get_config(self):\n",
    "        self.lower_edge_hertz = lower_edge_hertz\n",
    "        self.upper_edge_hertz = upper_edge_hertz\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "\n",
    "        config = {\n",
    "            'frame_length': self.frame_length,\n",
    "            'frame_step': self.frame_step,\n",
    "            'fft_length': self.fft_length,\n",
    "            'log_offset': self.log_offset,\n",
    "            'n_bins': self.n_bins,\n",
    "            'lower_edge_hertz': self.lower_edge_hertz,\n",
    "            'upper_edge_hertz': self.upper_edge_hertz,\n",
    "            'num_mel_bins': self.num_mel_bins,\n",
    "            'sr': self.sr\n",
    "        }\n",
    "        base_config = super(LogMelSpectrogram, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T14:50:49.343902Z",
     "start_time": "2018-01-16T14:50:48.412799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 100, 160]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "log_mel_spectrogram_1 (LogMe (None, 100, 160)          82080     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100, 160)          640       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 160)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100, 50)           8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100, 50)           200       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100)               30300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                3232      \n",
      "=================================================================\n",
      "Total params: 124,902\n",
      "Trainable params: 42,202\n",
      "Non-trainable params: 82,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SR = 16000\n",
    "N_SEQ = 100\n",
    "F_STEP = SR//(N_SEQ)\n",
    "F_LEN = 2*N_STEP\n",
    "FFT_LEN = max(2**(int(log2(N_STEP))+1),1024)\n",
    "N_MELS = 160\n",
    "DROP = .25\n",
    "INIT = \"he_normal\"\n",
    "ACT = \"elu\"\n",
    "N_CAT = int(ex_df.raw_label_i.max()+1)\n",
    "N_BATCH = 512\n",
    "VAL = [\"val\", \"test\"]\n",
    "TRAIN = [\"train\"]\n",
    "REG = None # keras.regularizers.l2(0.1)\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(SR,))\n",
    "input_block = LogMelSpectrogram(\n",
    "    frame_length=N_LEN,\n",
    "    frame_step=N_STEP,\n",
    "    fft_length=FFT_LEN,\n",
    "    lower_edge_hertz=80.0,\n",
    "    upper_edge_hertz=7600.0,\n",
    "    num_mel_bins=N_MELS,\n",
    "    sr=SR,\n",
    ")(input_layer)\n",
    "print(input_block.shape.as_list())\n",
    "\n",
    "input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "input_block = keras.layers.Dense(50, activation=ACT)(input_block)\n",
    "input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "rnn_block = input_block\n",
    "\n",
    "rnn_block = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(\n",
    "        50,\n",
    "        activation=ACT,\n",
    "        kernel_initializer=INIT,\n",
    "        dropout=DROP,\n",
    "        recurrent_dropout=DROP,\n",
    "        kernel_regularizer=REG, \n",
    "        recurrent_regularizer=REG, \n",
    "        bias_regularizer=REG\n",
    "    ), merge_mode=\"concat\")(rnn_block)\n",
    "rnn_block = keras.layers.BatchNormalization()(rnn_block)\n",
    "\n",
    "output_layer = keras.layers.Dense(\n",
    "    N_CAT, activation=\"softmax\", kernel_initializer=INIT)(rnn_block)\n",
    "rnn_model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "rnn_model.summary()\n",
    "rnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-16T14:50:54.213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "104/103 [==============================] - 46s 446ms/step - loss: 3.3349 - sparse_categorical_accuracy: 0.0869 - val_loss: 3.5623 - val_sparse_categorical_accuracy: 0.0365\n",
      "Epoch 2/200\n",
      "104/103 [==============================] - 42s 402ms/step - loss: 3.2431 - sparse_categorical_accuracy: 0.1003 - val_loss: 3.4624 - val_sparse_categorical_accuracy: 0.0386\n",
      "Epoch 3/200\n",
      "104/103 [==============================] - 42s 401ms/step - loss: 3.2806 - sparse_categorical_accuracy: 0.0897 - val_loss: 3.5421 - val_sparse_categorical_accuracy: 0.0323\n",
      "Epoch 4/200\n",
      "104/103 [==============================] - 42s 406ms/step - loss: 3.0366 - sparse_categorical_accuracy: 0.1396 - val_loss: 3.5067 - val_sparse_categorical_accuracy: 0.0582\n",
      "Epoch 5/200\n",
      "104/103 [==============================] - 42s 403ms/step - loss: 2.5314 - sparse_categorical_accuracy: 0.2529 - val_loss: 3.3913 - val_sparse_categorical_accuracy: 0.0750\n",
      "Epoch 6/200\n",
      "104/103 [==============================] - 42s 402ms/step - loss: 2.1025 - sparse_categorical_accuracy: 0.3658 - val_loss: 2.2686 - val_sparse_categorical_accuracy: 0.2842\n",
      "Epoch 7/200\n",
      "104/103 [==============================] - 42s 400ms/step - loss: 2.3847 - sparse_categorical_accuracy: 0.3008 - val_loss: 3.0620 - val_sparse_categorical_accuracy: 0.1687\n",
      "Epoch 8/200\n",
      "104/103 [==============================] - 42s 403ms/step - loss: 2.2926 - sparse_categorical_accuracy: 0.3244 - val_loss: 2.1799 - val_sparse_categorical_accuracy: 0.3299\n",
      "Epoch 9/200\n",
      "104/103 [==============================] - 42s 403ms/step - loss: 2.2135 - sparse_categorical_accuracy: 0.3461 - val_loss: 1.8506 - val_sparse_categorical_accuracy: 0.4298\n",
      "Epoch 10/200\n",
      "104/103 [==============================] - 43s 409ms/step - loss: 2.0660 - sparse_categorical_accuracy: 0.3789 - val_loss: 1.7438 - val_sparse_categorical_accuracy: 0.4665\n",
      "Epoch 11/200\n",
      "104/103 [==============================] - 42s 400ms/step - loss: 1.9746 - sparse_categorical_accuracy: 0.4084 - val_loss: 1.6978 - val_sparse_categorical_accuracy: 0.4913\n",
      "Epoch 12/200\n",
      "104/103 [==============================] - 42s 401ms/step - loss: 1.8862 - sparse_categorical_accuracy: 0.4361 - val_loss: 1.6458 - val_sparse_categorical_accuracy: 0.5134\n",
      "Epoch 13/200\n",
      "104/103 [==============================] - 43s 409ms/step - loss: 1.7601 - sparse_categorical_accuracy: 0.4761 - val_loss: 1.6008 - val_sparse_categorical_accuracy: 0.5153\n",
      "Epoch 14/200\n",
      "104/103 [==============================] - 42s 405ms/step - loss: 1.7215 - sparse_categorical_accuracy: 0.4778 - val_loss: 1.5566 - val_sparse_categorical_accuracy: 0.5356\n",
      "Epoch 15/200\n",
      "104/103 [==============================] - 42s 405ms/step - loss: 1.6946 - sparse_categorical_accuracy: 0.4972 - val_loss: 1.4617 - val_sparse_categorical_accuracy: 0.5609\n",
      "Epoch 16/200\n",
      "104/103 [==============================] - 42s 402ms/step - loss: 1.6305 - sparse_categorical_accuracy: 0.5051 - val_loss: 1.4971 - val_sparse_categorical_accuracy: 0.5649\n",
      "Epoch 17/200\n",
      "104/103 [==============================] - 42s 401ms/step - loss: 1.5293 - sparse_categorical_accuracy: 0.5417 - val_loss: 1.4192 - val_sparse_categorical_accuracy: 0.5946\n",
      "Epoch 18/200\n",
      "104/103 [==============================] - 43s 410ms/step - loss: 1.5254 - sparse_categorical_accuracy: 0.5447 - val_loss: 1.3218 - val_sparse_categorical_accuracy: 0.6146\n",
      "Epoch 19/200\n",
      "104/103 [==============================] - 43s 409ms/step - loss: 1.3968 - sparse_categorical_accuracy: 0.5686 - val_loss: 1.4516 - val_sparse_categorical_accuracy: 0.6142\n",
      "Epoch 20/200\n",
      "104/103 [==============================] - 42s 407ms/step - loss: 1.3417 - sparse_categorical_accuracy: 0.6056 - val_loss: 1.2180 - val_sparse_categorical_accuracy: 0.6452\n",
      "Epoch 21/200\n",
      "104/103 [==============================] - 42s 403ms/step - loss: 1.3155 - sparse_categorical_accuracy: 0.6061 - val_loss: 1.1254 - val_sparse_categorical_accuracy: 0.6534\n",
      "Epoch 22/200\n",
      "104/103 [==============================] - 42s 402ms/step - loss: 1.2222 - sparse_categorical_accuracy: 0.6291 - val_loss: 1.1445 - val_sparse_categorical_accuracy: 0.6534\n",
      "Epoch 23/200\n",
      "104/103 [==============================] - 42s 403ms/step - loss: 1.1680 - sparse_categorical_accuracy: 0.6485 - val_loss: 1.1913 - val_sparse_categorical_accuracy: 0.6736\n",
      "Epoch 24/200\n",
      "104/103 [==============================] - 42s 404ms/step - loss: 1.1423 - sparse_categorical_accuracy: 0.6541 - val_loss: 1.1388 - val_sparse_categorical_accuracy: 0.6906\n",
      "Epoch 25/200\n",
      "104/103 [==============================] - 42s 405ms/step - loss: 1.1582 - sparse_categorical_accuracy: 0.6505 - val_loss: 1.0945 - val_sparse_categorical_accuracy: 0.7049\n",
      "Epoch 26/200\n",
      "104/103 [==============================] - 42s 409ms/step - loss: 1.1298 - sparse_categorical_accuracy: 0.6560 - val_loss: 1.0945 - val_sparse_categorical_accuracy: 0.7150\n",
      "Epoch 27/200\n",
      "104/103 [==============================] - 42s 401ms/step - loss: 0.9828 - sparse_categorical_accuracy: 0.6982 - val_loss: 1.0269 - val_sparse_categorical_accuracy: 0.7135\n",
      "Epoch 28/200\n",
      " 73/103 [====================>.........] - ETA: 11s - loss: 0.9083 - sparse_categorical_accuracy: 0.7188"
     ]
    }
   ],
   "source": [
    "traing_gen = old_gen(\n",
    "    batch_size=N_BATCH,\n",
    "    raw_label=True,\n",
    "    state=TRAIN,\n",
    "    vol_range=.1,\n",
    "    p_transform=1,\n",
    "    shift=0\n",
    ")\n",
    "\n",
    "history = rnn_model.fit_generator(\n",
    "    generator=traing_gen,\n",
    "    steps_per_epoch=sum(ex_df.state.isin(TRAIN)) / N_BATCH,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:14:10.508394Z",
     "start_time": "2018-01-16T21:14:10.376755Z"
    }
   },
   "outputs": [],
   "source": [
    "class MFCC(keras.engine.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 frame_length=1024,\n",
    "                 frame_step=512,\n",
    "                 fft_length=1024,\n",
    "                 lower_edge_hertz=None,\n",
    "                 upper_edge_hertz=None,\n",
    "                 num_mel_bins=64,\n",
    "                 sr=16000,\n",
    "                 n_mfcc=13,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.fft_length = fft_length\n",
    "        self.n_bins = self.fft_length // 2 + 1\n",
    "        self.log_offset = 1e-6\n",
    "        if lower_edge_hertz:\n",
    "            self.lower_edge_hertz = lower_edge_hertz\n",
    "        else:\n",
    "            self.lower_edge_hertz = 0\n",
    "        if upper_edge_hertz:\n",
    "            self.upper_edge_hertz = upper_edge_hertz\n",
    "        else:\n",
    "            self.upper_edge_hertz = sr//2\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        super(MFCC, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
    "            self.num_mel_bins, self.n_bins, self.sr, self.lower_edge_hertz,\n",
    "            self.upper_edge_hertz)\n",
    "        \n",
    "        self.non_trainable_weights.append(self.linear_to_mel_weight_matrix)\n",
    "        super(MFCC, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch = input_shape[0]\n",
    "        n_samples = input_shape[1]\n",
    "        n_seq = n_samples//self.frame_step\n",
    "\n",
    "        return batch, n_seq, self.n_mfcc\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        stfts = tf.contrib.signal.stft(\n",
    "            x,\n",
    "            frame_length=self.frame_length,\n",
    "            frame_step=self.frame_step,\n",
    "            fft_length=self.fft_length,\n",
    "            pad_end=True,\n",
    "        )\n",
    "        magnitude_spectrograms = tf.abs(stfts)\n",
    "\n",
    "        mel_spectrograms = K.dot(magnitude_spectrograms, self.linear_to_mel_weight_matrix)\n",
    "\n",
    "        log_mel_spectrograms = tf.log(mel_spectrograms + self.log_offset)\n",
    "        \n",
    "        mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(\n",
    "                  log_mel_spectrograms\n",
    "        )[..., :self.n_mfcc]\n",
    "        \n",
    "        return mfccs\n",
    "\n",
    "    def get_config(self):\n",
    "        self.lower_edge_hertz = lower_edge_hertz\n",
    "        self.upper_edge_hertz = upper_edge_hertz\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "\n",
    "        config = {\n",
    "            'frame_length': self.frame_length,\n",
    "            'frame_step': self.frame_step,\n",
    "            'fft_length': self.fft_length,\n",
    "            'log_offset': self.log_offset,\n",
    "            'n_bins': self.n_bins,\n",
    "            'lower_edge_hertz': self.lower_edge_hertz,\n",
    "            'upper_edge_hertz': self.upper_edge_hertz,\n",
    "            'num_mel_bins': self.num_mel_bins,\n",
    "            'sr': self.sr,\n",
    "            'n_mfcc': n_mfcc\n",
    "        }\n",
    "        base_config = super(MFCC, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:14:11.540808Z",
     "start_time": "2018-01-16T21:14:11.452303Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeltaDelta(keras.engine.Layer):\n",
    "    '''\n",
    "    Layer that appends deltas as an extra channel\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n=2, order=2, **kwargs):\n",
    "        assert order==1 or order==2\n",
    "        self.n = n\n",
    "        self.order = order\n",
    "        super(DeltaDelta, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch = input_shape[0]\n",
    "        time = input_shape[1]\n",
    "        features = input_shape[2]\n",
    "\n",
    "\n",
    "        return batch, time, features, self.order+1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        delta_kernel = np.arange(-self.n, self.n + 1\n",
    "                                 ).reshape((1, 2 * self.n + 1, 1, 1))\n",
    "        delta_kernel = delta_kernel/(2*sum(np.arange(self.n+1)**2))\n",
    "\n",
    "        self.delta_kernel = K.variable(delta_kernel, dtype=K.floatx())\n",
    "\n",
    "        self.non_trainable_weights.append(self.delta_kernel)\n",
    "        self.paddings = K.constant([[0,0], [0, 0], [self.n, self.n], [0,0]], dtype=\"int32\")\n",
    "        super(DeltaDelta, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        x_orig = tf.expand_dims(x, -1)\n",
    "        deltas = [x_orig]\n",
    "        \n",
    "        to_delta = x_orig\n",
    "        for i in range(self.order):\n",
    "            x_pad = tf.pad(to_delta, self.paddings)\n",
    "            delta = K.conv2d(x_pad, self.delta_kernel, data_format=\"channels_last\")\n",
    "            deltas.append(delta)\n",
    "            to_delta = delta\n",
    "\n",
    "        return K.concatenate(deltas, axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n': self.n, 'order': self.order}\n",
    "        base_config = super(DeltaDelta, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:14:51.006742Z",
     "start_time": "2018-01-16T21:14:41.530994Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 64, 64]\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)                         (None, 16000)                           0              \n",
      "____________________________________________________________________________________________________\n",
      "mfcc_1 (MFCC)                                (None, 64, 64)                          32832          \n",
      "____________________________________________________________________________________________________\n",
      "delta_delta_1 (DeltaDelta)                   (None, 64, 64, 3)                       5              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                          (None, 64, 64, 3)                       0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                            (None, 64, 64, 20)                      15380          \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)               (None, 32, 32, 20)                      0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization)   (None, 32, 32, 20)                      80             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)                          (None, 32, 32, 20)                      0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                            (None, 32, 32, 20)                      25620          \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)               (None, 16, 16, 20)                      0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization)   (None, 16, 16, 20)                      80             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)                          (None, 16, 16, 20)                      0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                            (None, 16, 16, 20)                      6420           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)               (None, 8, 8, 20)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization)   (None, 8, 8, 20)                        80             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)                          (None, 8, 8, 20)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                            (None, 8, 8, 30)                        2430           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)               (None, 4, 4, 30)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization)   (None, 4, 4, 30)                        120            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)                          (None, 4, 4, 30)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                            (None, 4, 4, 40)                        4840           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)               (None, 2, 2, 40)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization)   (None, 2, 2, 40)                        160            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)                          (None, 2, 2, 40)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                            (None, 2, 2, 50)                        8050           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)               (None, 1, 1, 50)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization)   (None, 1, 1, 50)                        200            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)                          (None, 1, 1, 50)                        0              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)                          (None, 50)                              0              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                              (None, 32)                              1632           \n",
      "====================================================================================================\n",
      "Total params: 97,929\n",
      "Trainable params: 64,732\n",
      "Non-trainable params: 33,197\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "  9/103 [=>............................] - ETA: 1:08 - loss: 4.2116 - sparse_categorical_accuracy: 0.0319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ca1f50c0f73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SR = 16000\n",
    "N_SEQ = 64\n",
    "F_STEP = SR//(N_SEQ)\n",
    "F_LEN = 2*F_STEP\n",
    "FFT_LEN = max(2**(int(log2(F_STEP))+1),1024)\n",
    "N_MELS = 64\n",
    "DROP = .2\n",
    "INIT = \"he_normal\"\n",
    "ACT = \"elu\"\n",
    "N_MFCC = N_MELS\n",
    "D_ORDER = 2\n",
    "REG = None\n",
    "N_CAT = int(ex_df.raw_label_i.max()+1)\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(SR,))\n",
    "input_layer = keras.layers.Input(shape=(SR,))\n",
    "input_block = MFCC(\n",
    "    frame_length=F_LEN,\n",
    "    frame_step=F_STEP,\n",
    "    fft_length=FFT_LEN,\n",
    "    num_mel_bins=N_MELS,\n",
    "    sr=SR,\n",
    "    n_mfcc=N_MFCC,\n",
    "    upper_edge_hertz=4000,\n",
    "    lower_edge_hertz=40\n",
    ")(input_layer)\n",
    "print(input_block.shape.as_list())\n",
    "input_block = DeltaDelta()(input_block)\n",
    "\n",
    "# input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "\n",
    "cnn_block = input_block \n",
    "\n",
    "cnn_block = keras.layers.Conv2D(20, 16, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "cnn_block = keras.layers.Conv2D(20, 8, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "cnn_block = keras.layers.Conv2D(20, 4, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "\n",
    "cnn_block = keras.layers.Conv2D(30, 2, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "cnn_block = keras.layers.Conv2D(40, 2, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "cnn_block = keras.layers.Conv2D(50, 2, padding=\"same\", activation=ACT)(cnn_block)\n",
    "cnn_block = keras.layers.MaxPool2D()(cnn_block)\n",
    "cnn_block = keras.layers.BatchNormalization()(cnn_block)\n",
    "cnn_block = keras.layers.Dropout(DROP)(cnn_block)\n",
    "\n",
    "output_block = keras.layers.Flatten()(cnn_block)\n",
    "\n",
    "output_layer = keras.layers.Dense(\n",
    "    N_CAT, activation=\"softmax\", kernel_initializer=INIT)(output_block)\n",
    "cnn_model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "cnn_model.summary(line_length=100)\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "history = cnn_model.fit_generator(\n",
    "    generator=traing_gen,\n",
    "    steps_per_epoch=sum(ex_df.state.isin(TRAIN)) / N_BATCH,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:26:22.426016Z",
     "start_time": "2018-01-16T21:26:22.422383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T21:33:13.643503Z",
     "start_time": "2018-01-16T21:33:12.956175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "input_10 (InputLayer)                        (None, 16000)                           0              \n",
      "____________________________________________________________________________________________________\n",
      "mfcc_9 (MFCC)                                (None, 100, 13)                         10280          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNormalization)  (None, 100, 13)                         52             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)                         (None, 100, 13)                         0              \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional)             (None, 50)                              5850           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNormalization)  (None, 50)                              200            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                              (None, 32)                              1632           \n",
      "====================================================================================================\n",
      "Total params: 18,014\n",
      "Trainable params: 7,608\n",
      "Non-trainable params: 10,406\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SR = 16000\n",
    "N_SEQ = 100\n",
    "F_STEP = 160\n",
    "F_LEN = 400\n",
    "FFT_LEN = 512\n",
    "N_MELS = 40\n",
    "DROP = .1\n",
    "INIT = \"he_normal\"\n",
    "ACT = \"elu\"\n",
    "N_MFCC = 13\n",
    "D_ORDER = 2\n",
    "REG = None\n",
    "N_CAT = int(ex_df.raw_label_i.max()+1)\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(SR,))\n",
    "input_block = MFCC(\n",
    "    frame_length=F_LEN,\n",
    "    frame_step=F_STEP,\n",
    "    fft_length=FFT_LEN,\n",
    "    num_mel_bins=N_MELS,\n",
    "    upper_edge_hertz=4000,\n",
    "    lower_edge_hertz=40,\n",
    "    sr=SR,\n",
    "    n_mfcc=N_MFCC,\n",
    ")(input_layer)\n",
    "# input_block = DeltaDelta()(input_block)\n",
    "# input_block = keras.layers.Reshape((N_SEQ, N_MFCC*(D_ORDER+1)))(input_block)\n",
    "\n",
    "input_block = keras.layers.BatchNormalization()(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "\n",
    "\n",
    "rnn_block = input_block\n",
    "\n",
    "# rnn_block = keras.layers.Bidirectional(\n",
    "#     keras.layers.GRU(\n",
    "#         32,\n",
    "#         activation=ACT,\n",
    "#         kernel_initializer=INIT,\n",
    "#         dropout=DROP,\n",
    "#         recurrent_dropout=DROP,\n",
    "#         kernel_regularizer=REG, \n",
    "#         recurrent_regularizer=REG, \n",
    "#         bias_regularizer=REG,\n",
    "#         return_sequences=True\n",
    "#     ), merge_mode=\"concat\")(rnn_block)\n",
    "# rnn_block = keras.layers.BatchNormalization()(rnn_block)\n",
    "\n",
    "rnn_block = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(\n",
    "        25,\n",
    "        activation=ACT,\n",
    "        kernel_initializer=INIT,\n",
    "        dropout=DROP,\n",
    "        recurrent_dropout=DROP,\n",
    "        kernel_regularizer=REG, \n",
    "        recurrent_regularizer=REG, \n",
    "        bias_regularizer=REG\n",
    "    ), merge_mode=\"concat\")(rnn_block)\n",
    "rnn_block = keras.layers.BatchNormalization()(rnn_block)\n",
    "\n",
    "output_layer = keras.layers.Dense(\n",
    "    N_CAT, activation=\"softmax\", kernel_initializer=INIT)(rnn_block)\n",
    "rnn_model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "rnn_model.summary(line_length=100)\n",
    "rnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-16T23:03:42.496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 17/103 [===>..........................] - ETA: 1:19 - loss: 0.6702 - sparse_categorical_accuracy: 0.8009"
     ]
    }
   ],
   "source": [
    "traing_gen = old_gen(\n",
    "    batch_size=N_BATCH,\n",
    "    raw_label=True,\n",
    "    state=TRAIN,\n",
    "    vol_range=.1,\n",
    "    p_transform=.5,\n",
    "    shift=.5\n",
    ")\n",
    "\n",
    "history = rnn_model.fit_generator(\n",
    "    generator=traing_gen,\n",
    "    steps_per_epoch=sum(ex_df.state.isin(TRAIN)) / N_BATCH,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC RNN\n",
    "\n",
    "That RNN is pretty good. I've designed it to be sparse and super fastit does an epoch of ~60k examples in 13s and has only 12k trainable params. And it still gets ~80% validation accuracy! \n",
    "\n",
    "Now use as similar architecture to build a CTC-based RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T02:52:08.014695Z",
     "start_time": "2018-01-16T02:52:06.021889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "{'bed': ['B', 'EH', 'D'],\n",
      " 'bird': ['B', 'ER', 'D'],\n",
      " 'cat': ['K', 'AE', 'T'],\n",
      " 'dog': ['D', 'AO', 'G'],\n",
      " 'down': ['D', 'AW', 'N'],\n",
      " 'eight': ['EY', 'T'],\n",
      " 'five': ['F', 'AY', 'V'],\n",
      " 'four': ['F', 'AO', 'R'],\n",
      " 'go': ['G', 'OW'],\n",
      " 'happy': ['HH', 'AE', 'P', 'IY'],\n",
      " 'house': ['HH', 'AW', 'S'],\n",
      " 'left': ['L', 'EH', 'F', 'T'],\n",
      " 'marvin': ['M', 'AA', 'R', 'V', 'IH', 'N'],\n",
      " 'nine': ['N', 'AY', 'N'],\n",
      " 'no': ['N', 'OW'],\n",
      " 'off': ['AO', 'F'],\n",
      " 'on': ['AA', 'N'],\n",
      " 'one': ['W', 'AH', 'N'],\n",
      " 'right': ['R', 'AY', 'T'],\n",
      " 'seven': ['S', 'EH', 'V', 'AH', 'N'],\n",
      " 'sheila': ['SH', 'IY', 'L', 'AH'],\n",
      " 'six': ['S', 'IH', 'K', 'S'],\n",
      " 'stop': ['S', 'T', 'AA', 'P'],\n",
      " 'three': ['TH', 'R', 'IY'],\n",
      " 'tree': ['T', 'R', 'IY'],\n",
      " 'two': ['T', 'UW'],\n",
      " 'up': ['AH', 'P'],\n",
      " 'wow': ['W', 'AW'],\n",
      " 'yes': ['Y', 'EH', 'S'],\n",
      " 'zero': ['Z', 'IH', 'R', 'OW']}\n",
      "33 phonemes in alphabet\n",
      "['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'D', 'EH', 'ER', 'EY', 'F', 'G', 'HH',\n",
      " 'IH', 'IY', 'K', 'L', 'M', 'N', 'OW', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UW',\n",
      " 'V', 'W', 'Y', 'Z', '-']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('cmudict')\n",
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "\n",
    "words = list(ex_df.raw_label.unique())\n",
    "words.remove(np.nan)\n",
    "words.remove(\"silence\")\n",
    "\n",
    "phone_dict = dict()\n",
    "phone_set = set()\n",
    "maxlen = 0\n",
    "for w in words:\n",
    "    phones = arpabet[w][0]\n",
    "    phones = [p.strip(\"01\") for p in phones] #remove emphasis on vowels\n",
    "    phones_b = phones\n",
    "    phone_dict[w] = phones_b\n",
    "    phone_set |= set(phones)\n",
    "    if (len(phones_b)) > maxlen:\n",
    "        maxlen = len(phones_b)\n",
    "# phone_dict[\"silence\"] = [\"-\"]\n",
    "alphabet = sorted(list(phone_set)) + [\"-\"]\n",
    "\n",
    "def text_to_labels(text):\n",
    "    phones = phone_dict[text]\n",
    "    ret = [alphabet.index(p) for p in phones]\n",
    "    return ret\n",
    "\n",
    "N_CAT = len(alphabet)\n",
    "pprint.pprint(phone_dict, compact=True)\n",
    "print(\"{} phonemes in alphabet\".format(N_CAT))\n",
    "\n",
    "pprint.pprint(alphabet, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T02:52:19.627741Z",
     "start_time": "2018-01-16T02:52:19.623614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero [31, 14, 22, 20]\n",
      "silence [29, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"zero\", text_to_labels(\"zero\"))\n",
    "print(\"silence\", text_to_labels(\"wow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T02:52:38.598550Z",
     "start_time": "2018-01-16T02:52:38.542826Z"
    }
   },
   "outputs": [],
   "source": [
    "def ex_generator(\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        state=\"train\",\n",
    "        num_seq=None,\n",
    "        input_len=10,\n",
    "        p_transform=0,\n",
    "        vol_range=0,\n",
    "        shift=0,\n",
    "):\n",
    "\n",
    "    epoch_df = ex_df[ex_df.state.isin(state)&(ex_df.raw_label != \"silence\")]\n",
    "    num_ex = len(epoch_df)\n",
    "    indices = np.arange(num_ex)\n",
    "\n",
    "    # epoch loop runs\n",
    "    while True:\n",
    "\n",
    "        # shuffle anew every epoch\n",
    "        if shuffle:\n",
    "            epoch_df = epoch_df.sample(frac=1)\n",
    "\n",
    "        # batch loop\n",
    "        for i in np.arange(0, num_ex, batch_size):\n",
    "\n",
    "            batch_df = epoch_df.iloc[i:i + batch_size, :]\n",
    "\n",
    "            x = np.zeros((len(batch_df), 16000))\n",
    "            labels = np.zeros((len(batch_df), maxlen))\n",
    "            label_len = np.zeros((len(batch_df), 1))\n",
    "\n",
    "            # example loop\n",
    "            for b in range(len(batch_df)):\n",
    "\n",
    "                x[b, ...] = center_wave(\n",
    "                    epoch_df.fn.values[b],\n",
    "                    vol_range=vol_range,\n",
    "                    shift=shift,\n",
    "                    p_transform=p_transform)\n",
    "\n",
    "                labels_i = text_to_labels(epoch_df.raw_label.values[b])\n",
    "                label_len[b] = len(labels_i)\n",
    "                labels[b, :len(labels_i)] = labels_i\n",
    "\n",
    "            inputs = {\n",
    "                'wav': x,\n",
    "                'labels': labels,\n",
    "                'input_len': np.full((len(batch_df), 1), N_SEQ),\n",
    "                'label_len': label_len\n",
    "            }\n",
    "            outputs = {\n",
    "                'ctc': np.zeros([len(batch_df)]),\n",
    "                'ler': np.zeros([len(batch_df)])\n",
    "            }  # dummy data for dummy loss function\n",
    "\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T04:58:12.329975Z",
     "start_time": "2018-01-16T04:58:11.339664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 100, 160]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "wav (InputLayer)                (None, 16000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mfcc_23 (MFCC)                  (None, 100, 160)     82080       wav[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "delta_delta_22 (DeltaDelta)     (None, 100, 160, 3)  5           mfcc_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 100, 480)     0           delta_delta_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 100, 480)     960         reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 100, 480)     0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 100, 50)      24050       dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 100, 50)      100         dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 100, 50)      0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 100, 100)     30300       dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 100, 100)     200         bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_len (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 100, 33)      3333        batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_len (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           labels[0][0]                     \n",
      "                                                                 label_len[0][0]                  \n",
      "                                                                 logits[0][0]                     \n",
      "                                                                 input_len[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 141,028\n",
      "Trainable params: 57,683\n",
      "Non-trainable params: 83,345\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SR = 16000\n",
    "N_SEQ = 100\n",
    "F_STEP = SR // (N_SEQ)\n",
    "F_LEN = 2 * N_STEP\n",
    "FFT_LEN = max(2**(int(log2(N_STEP)) + 1), 1024)\n",
    "N_MELS = 160\n",
    "DROP = .25\n",
    "INIT = \"he_normal\"\n",
    "ACT = \"elu\"\n",
    "N_MFCC = N_MELS\n",
    "D_ORDER = 2\n",
    "REG = None\n",
    "\n",
    "input_layer = keras.layers.Input(shape=(SR, ), name='wav')\n",
    "input_block = MFCC(\n",
    "    frame_length=N_LEN,\n",
    "    frame_step=N_STEP,\n",
    "    fft_length=FFT_LEN,\n",
    "    num_mel_bins=N_MELS,\n",
    "    sr=SR,\n",
    "    n_mfcc=N_MFCC,\n",
    ")(input_layer)\n",
    "print(input_block.shape.as_list())\n",
    "input_block = DeltaDelta()(input_block)\n",
    "input_block = keras.layers.Reshape((N_SEQ,\n",
    "                                    N_MFCC * (D_ORDER + 1)))(input_block)\n",
    "\n",
    "input_block = keras.layers.BatchNormalization(center=False, scale=False)(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "input_block = keras.layers.Dense(\n",
    "    50,\n",
    "    activation=ACT,\n",
    "    kernel_initializer=INIT,\n",
    "    kernel_regularizer=REG,\n",
    "    bias_regularizer=REG\n",
    ")(input_block)\n",
    "input_block = keras.layers.BatchNormalization(center=False, scale=False)(input_block)\n",
    "input_block = keras.layers.Dropout(DROP)(input_block)\n",
    "\n",
    "rnn_block = input_block\n",
    "\n",
    "rnn_block = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(\n",
    "        50,\n",
    "        activation=ACT,\n",
    "        kernel_initializer=INIT,\n",
    "        dropout=DROP,\n",
    "        recurrent_dropout=DROP,\n",
    "        kernel_regularizer=REG,\n",
    "        recurrent_regularizer=REG,\n",
    "        bias_regularizer=REG,\n",
    "        return_sequences=True,\n",
    "    ),\n",
    "    merge_mode=\"concat\")(rnn_block)\n",
    "rnn_block = keras.layers.BatchNormalization(center=False, scale=False)(rnn_block)\n",
    "\n",
    "# def ctc_lambda_func(args):\n",
    "#     y_pred, labels, input_length, label_length = args\n",
    "#     return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "## OUT BLOCK\n",
    "logits = keras.layers.Dense(N_CAT, kernel_initializer=INIT, name='logits')(rnn_block)\n",
    "\n",
    "# these inputs are for the CTC loss\n",
    "labels = keras.layers.Input(name='labels', shape=[maxlen], dtype='float32')\n",
    "input_len = keras.layers.Input(name='input_len', shape=[1], dtype='int64')\n",
    "label_len = keras.layers.Input(name='label_len', shape=[1], dtype='int64')\n",
    "\n",
    "# LOSS\n",
    "def ctc_loss(args):\n",
    "    labels, label_len, logits, input_len = args\n",
    "    input_len = tf.to_int32(tf.squeeze(input_len))\n",
    "    label_len = tf.to_int32(tf.squeeze(label_len))\n",
    "    sparse_labels = tf.to_int32(K.ctc_label_dense_to_sparse(labels, label_len))\n",
    "\n",
    "    logits = tf.transpose(logits, perm=[1, 0, 2])\n",
    "\n",
    "    return tf.expand_dims(\n",
    "        tf.nn.ctc_loss(\n",
    "            inputs=logits, labels=sparse_labels, sequence_length=input_len), 1)\n",
    "\n",
    "loss_out = keras.layers.Lambda(\n",
    "    ctc_loss, output_shape=(1, ),\n",
    "    name='ctc')([labels, label_len, logits, input_len])\n",
    "\n",
    "\n",
    "\n",
    "# def get_ler(args):\n",
    "#     labels, label_len, logits, input_len = args\n",
    "#     input_len = tf.to_int32(tf.squeeze(input_len))\n",
    "#     batch_n = K.shape(input_len)[0]\n",
    "#     print(batch_n)\n",
    "\n",
    "#     decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, input_len)\n",
    "#     # Inaccuracy: label error rate\n",
    "\n",
    "#     label_len = tf.to_int32(tf.squeeze(label_len))\n",
    "#     sparse_labels =  tf.to_int32(K.ctc_label_dense_to_sparse(labels, label_len))\n",
    "\n",
    "#     ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "#                                           sparse_labels))\n",
    "    \n",
    "#     return ler\n",
    "\n",
    "# ler_out = keras.layers.Lambda(\n",
    "#     get_ler,\n",
    "#     name=\"ler\"\n",
    "# )([labels, label_len, logits, input_len])\n",
    "\n",
    "get_logits = K.function([input_layer], [logits])\n",
    "\n",
    "\n",
    "ctc_model = keras.Model(\n",
    "    inputs=[input_layer, labels, input_len, label_len], outputs=loss_out)\n",
    "ctc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:49:16.453647Z",
     "start_time": "2018-01-16T05:37:25.210242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/99 [==============================] - 79s 787ms/step - loss: 32.9800 - val_loss: 43.1413\n",
      "Epoch 2/10\n",
      "100/99 [==============================] - 69s 695ms/step - loss: 23.7920 - val_loss: 44.8456\n",
      "Epoch 3/10\n",
      "100/99 [==============================] - 70s 695ms/step - loss: inf - val_loss: inf\n",
      "Epoch 4/10\n",
      "100/99 [==============================] - 69s 695ms/step - loss: inf - val_loss: inf\n",
      "Epoch 5/10\n",
      "99/99 [============================>.] - ETA: 0s - loss: inf\n",
      "Epoch 00005: reducing learning rate to 0.0010000000474974513.\n",
      "100/99 [==============================] - 72s 717ms/step - loss: inf - val_loss: 49.9208\n",
      "Epoch 6/10\n",
      "100/99 [==============================] - 69s 694ms/step - loss: 23.7663 - val_loss: 24.5300\n",
      "Epoch 7/10\n",
      "100/99 [==============================] - 70s 696ms/step - loss: 18.5678 - val_loss: 24.0537\n",
      "Epoch 8/10\n",
      "100/99 [==============================] - 69s 694ms/step - loss: 22.0502 - val_loss: 32.6930\n",
      "Epoch 9/10\n",
      "100/99 [==============================] - 70s 695ms/step - loss: 20.8743 - val_loss: 24.7061\n",
      "Epoch 10/10\n",
      "100/99 [==============================] - 69s 695ms/step - loss: 18.7886 - val_loss: 24.6283\n"
     ]
    }
   ],
   "source": [
    "## MODEL\n",
    "\n",
    "val_data = next(ex_generator(\n",
    "    batch_size=sum(ex_df.state.isin(VAL)&(ex_df.raw_label != \"silence\")),\n",
    "    shuffle=False,\n",
    "    state=VAL))\n",
    "\n",
    "train_gen = ex_generator(\n",
    "    batch_size=N_BATCH,\n",
    "    state=TRAIN,\n",
    "    vol_range=.1,\n",
    "    p_transform=1,\n",
    "    shift=0\n",
    ")\n",
    "\n",
    "def acc(y_true, y_pred): \n",
    "    return y_pred\n",
    "\n",
    "ctc_model.compile(\n",
    "    loss={\n",
    "        \"ctc\": lambda y_true, y_pred: y_pred\n",
    "         },\n",
    "    optimizer=\"nadam\",\n",
    "#     metrics={\"ler\": acc}\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=6, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=.5, patience=3, verbose=1, min_lr=1e-8)\n",
    "]\n",
    "\n",
    "history = ctc_model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=sum(ex_df.state.isin(TRAIN)&(ex_df.raw_label != \"silence\")) / N_BATCH,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:03:34.586478Z",
     "start_time": "2018-01-16T05:03:34.454048Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = next(ex_generator(\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    state=VAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:04:32.513868Z",
     "start_time": "2018-01-16T05:04:32.509965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_len', 'wav', 'label_len'])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:09:33.663048Z",
     "start_time": "2018-01-16T05:09:33.456882Z"
    }
   },
   "outputs": [],
   "source": [
    "logits_arr = test_func([test_data[0]['wav']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:09:42.209808Z",
     "start_time": "2018-01-16T05:09:42.205913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 100, 33)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:33:27.705184Z",
     "start_time": "2018-01-16T05:33:27.686795Z"
    }
   },
   "outputs": [],
   "source": [
    "input_len = K.variable(value=np.full((512,1),100), dtype='int64')\n",
    "logits = K.variable(value=logits_arr, dtype='float32')\n",
    "labels = K.variable(value=test_data[0]['labels'], dtype='float32')\n",
    "label_len = K.variable(value=test_data[0]['label_len'], dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:33:28.560249Z",
     "start_time": "2018-01-16T05:33:28.553175Z"
    }
   },
   "outputs": [],
   "source": [
    "input_len = tf.to_int32(tf.squeeze(input_len))\n",
    "logits = K.permute_dimensions(logits, (1,0,2))\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:33:29.479055Z",
     "start_time": "2018-01-16T05:33:29.378681Z"
    }
   },
   "outputs": [],
   "source": [
    "label_len = tf.to_int32(tf.squeeze(label_len))\n",
    "sparse_labels =  tf.to_int32(K.ctc_label_dense_to_sparse(labels, label_len))\n",
    "dist = tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          sparse_labels)\n",
    "ler = tf.reduce_mean(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:33:34.919836Z",
     "start_time": "2018-01-16T05:33:30.528928Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:34:08.991822Z",
     "start_time": "2018-01-16T05:34:08.374436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1601562\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(ler.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:21:42.616608Z",
     "start_time": "2018-01-16T05:21:42.344816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Tensor(\"ler_9/Mean:0\", dtype=float32)\n",
      "None\n",
      "Tensor(\"ler_9/Mean_1:0\", dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-48793a3197d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mler_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mler_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mler_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                         target = K.placeholder(ndim=len(shape),\n\u001b[0m\u001b[1;32m    753\u001b[0m                                                \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                                                \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "labels = keras.layers.Input(name='labels', shape=[maxlen], dtype='float32')\n",
    "input_len = keras.layers.Input(name='input_len', shape=[1], dtype='int64')\n",
    "label_len = keras.layers.Input(name='label_len', shape=[1], dtype='int64')\n",
    "logits = keras.layers.Input(name='logits', shape=[N_SEQ, N_CAT], dtype='float32')\n",
    "\n",
    "def get_ler(args):\n",
    "    labels, label_len, logits, input_len = args\n",
    "    input_len = tf.to_int32(tf.squeeze(input_len))\n",
    "    batch_n = K.shape(input_len)[0]\n",
    "\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, input_len)\n",
    "    # Inaccuracy: label error rate\n",
    "\n",
    "    label_len = tf.to_int32(tf.squeeze(label_len))\n",
    "    sparse_labels =  tf.to_int32(K.ctc_label_dense_to_sparse(labels, label_len))\n",
    "\n",
    "    dist = tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          sparse_labels)\n",
    "    \n",
    "    print(K.int_shape(dist))\n",
    "    ler = tf.reduce_mean(dist)\n",
    "    print(ler)\n",
    "    return tf.expand_dims(ler,1)\n",
    "\n",
    "ler_out = keras.layers.Lambda(\n",
    "    get_ler,\n",
    "    name=\"ler\"\n",
    ")([labels, label_len, logits, input_len])\n",
    "\n",
    "ler_model = keras.Model(inputs=[labels, input_len, label_len, logits], outputs=ler_out)\n",
    "\n",
    "ler_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:18:19.940742Z",
     "start_time": "2018-01-16T05:18:19.936696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['input_len'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:16:38.971533Z",
     "start_time": "2018-01-16T05:16:38.968386Z"
    }
   },
   "outputs": [],
   "source": [
    "x = test_data[0]\n",
    "x[\"logits\"] = logits_arr\n",
    "y = {\"ler\":test_data[1]['ctc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:18:32.598851Z",
     "start_time": "2018-01-16T05:18:32.567466Z"
    }
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "len(sequence_length) != batch_size.  len(sequence_length):  512 batch_size: 100\n\t [[Node: ler_3/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_logits_2_0_3, ler_3/ToInt32/_4811)]]\n\t [[Node: ler_3/ToInt64_1/_4858 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_150_ler_3/ToInt64_1\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ler_3/ToInt64_1)]]\n\nCaused by op 'ler_3/CTCGreedyDecoder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-183-409dc4287154>\", line 24, in <module>\n    )([labels, label_len, logits, input_len])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/core.py\", line 651, in call\n    return self.function(inputs, **arguments)\n  File \"<ipython-input-183-409dc4287154>\", line 11, in get_ler\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, input_len)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/ctc_ops.py\", line 221, in ctc_greedy_decoder\n    inputs, sequence_length, merge_repeated=merge_repeated)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_ctc_ops.py\", line 147, in _ctc_greedy_decoder\n    merge_repeated=merge_repeated, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): len(sequence_length) != batch_size.  len(sequence_length):  512 batch_size: 100\n\t [[Node: ler_3/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_logits_2_0_3, ler_3/ToInt32/_4811)]]\n\t [[Node: ler_3/ToInt64_1/_4858 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_150_ler_3/ToInt64_1\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ler_3/ToInt64_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: len(sequence_length) != batch_size.  len(sequence_length):  512 batch_size: 100\n\t [[Node: ler_3/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_logits_2_0_3, ler_3/ToInt32/_4811)]]\n\t [[Node: ler_3/ToInt64_1/_4858 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_150_ler_3/ToInt64_1\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ler_3/ToInt64_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-6ec2882b4db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mler_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: len(sequence_length) != batch_size.  len(sequence_length):  512 batch_size: 100\n\t [[Node: ler_3/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_logits_2_0_3, ler_3/ToInt32/_4811)]]\n\t [[Node: ler_3/ToInt64_1/_4858 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_150_ler_3/ToInt64_1\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ler_3/ToInt64_1)]]\n\nCaused by op 'ler_3/CTCGreedyDecoder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-183-409dc4287154>\", line 24, in <module>\n    )([labels, label_len, logits, input_len])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/core.py\", line 651, in call\n    return self.function(inputs, **arguments)\n  File \"<ipython-input-183-409dc4287154>\", line 11, in get_ler\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, input_len)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/ctc_ops.py\", line 221, in ctc_greedy_decoder\n    inputs, sequence_length, merge_repeated=merge_repeated)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_ctc_ops.py\", line 147, in _ctc_greedy_decoder\n    merge_repeated=merge_repeated, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): len(sequence_length) != batch_size.  len(sequence_length):  512 batch_size: 100\n\t [[Node: ler_3/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_logits_2_0_3, ler_3/ToInt32/_4811)]]\n\t [[Node: ler_3/ToInt64_1/_4858 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_150_ler_3/ToInt64_1\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ler_3/ToInt64_1)]]\n"
     ]
    }
   ],
   "source": [
    "ler_model.predict(x, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T05:16:14.162618Z",
     "start_time": "2018-01-16T05:16:14.154536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]['ctc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T04:54:58.007665Z",
     "start_time": "2018-01-16T04:54:58.004881Z"
    }
   },
   "outputs": [],
   "source": [
    "layer_test = ctc_model.layers_by_depth[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T04:55:45.637276Z",
     "start_time": "2018-01-16T04:55:45.633239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ctc_8/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_test.get_output_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T23:00:54.201949Z",
     "start_time": "2018-01-11T23:00:54.190768Z"
    }
   },
   "outputs": [],
   "source": [
    "ctc_pred = K.function([input_layer], [y_pred])\n",
    "\n",
    "def decode_batch(test_func, word_batch):\n",
    "    out = test_func([word_batch])[0]\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = labels_to_text(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T03:07:08.215845Z",
     "start_time": "2018-01-16T03:06:58.841114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13633/13633 [==============================] - 9s 687us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20.72673 ],\n",
       "       [16.544771],\n",
       "       [20.867598],\n",
       "       ...,\n",
       "       [19.715319],\n",
       "       [18.101973],\n",
       "       [19.872498]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_model.predict(val_data[0], batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T02:59:12.923285Z",
     "start_time": "2018-01-16T02:59:12.916837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_len': array([[100],\n",
       "         [100],\n",
       "         [100],\n",
       "         ...,\n",
       "         [100],\n",
       "         [100],\n",
       "         [100]]), 'label_len': array([[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         ...,\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]), 'labels': array([[22.,  5., 25.,  0.,  0.,  0.],\n",
       "         [22.,  5., 25.,  0.,  0.,  0.],\n",
       "         [22.,  5., 25.,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [11.,  3., 22.,  0.,  0.,  0.],\n",
       "         [11.,  3., 22.,  0.,  0.,  0.],\n",
       "         [11.,  3., 22.,  0.,  0.,  0.]]), 'wav': array([[ 3.59990406e-07,  3.89989607e-07,  3.39990939e-07, ...,\n",
       "           2.99992005e-07,  3.59990406e-07,  3.79989873e-07],\n",
       "         [-3.99894024e-08, -4.89870180e-07, -5.09864881e-07, ...,\n",
       "          -5.79846335e-07, -4.79872829e-07, -3.49907271e-07],\n",
       "         [-5.99836401e-08, -5.99836401e-08, -5.99836401e-08, ...,\n",
       "          -1.09970007e-07, -2.09942740e-07, -3.49904567e-07],\n",
       "         ...,\n",
       "         [-1.89965367e-07, -1.49972658e-07,  2.49954431e-07, ...,\n",
       "           3.19941671e-07,  3.39938026e-07,  3.49936203e-07],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-3.99934931e-08,  2.99951198e-08,  2.99951198e-08, ...,\n",
       "           9.99837326e-08,  1.89969092e-07,  1.89969092e-07]])},\n",
       " {'ctc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  'ler': array([0., 0., 0., ..., 0., 0., 0.])})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T23:00:45.662964Z",
     "start_time": "2018-01-11T23:00:45.637259Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ex = ex_df[ex_df.state==\"test\"].sample(n=1)\n",
    "\n",
    "label = test_ex.raw_label.values[0]\n",
    "fn = test_ex.fn.values[0]\n",
    "wav = center_wave(fn)\n",
    "\n",
    "test_batch = np.zeros((1,1,16000))\n",
    "test_batch[0,...] = wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T23:03:31.885455Z",
     "start_time": "2018-01-11T23:03:31.882699Z"
    }
   },
   "outputs": [],
   "source": [
    "ctc_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T23:03:33.097711Z",
     "start_time": "2018-01-11T23:03:33.062493Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'batch_normalization_1/keras_learning_phase', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b8de7b69af0a>\", line 7, in <module>\n    \"DeltaDelta\": kapre.utils.DeltaDelta,\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 240, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 314, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\", line 140, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2500, in from_config\n    process_node(layer, node_data)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2457, in process_node\n    layer(input_tensors[0], **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2740, in in_train_phase\n    training = learning_phase()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 121, in learning_phase\n    name='keras_learning_phase')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5855a199a38d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctc_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-cd4f262535ca>\u001b[0m in \u001b[0;36mdecode_batch\u001b[0;34m(test_func, word_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'batch_normalization_1/keras_learning_phase', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b8de7b69af0a>\", line 7, in <module>\n    \"DeltaDelta\": kapre.utils.DeltaDelta,\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 240, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 314, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\", line 140, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2500, in from_config\n    process_node(layer, node_data)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2457, in process_node\n    layer(input_tensors[0], **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2740, in in_train_phase\n    training = learning_phase()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 121, in learning_phase\n    name='keras_learning_phase')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n\t [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "decode_batch(ctc_pred, test_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T23:03:21.927650Z",
     "start_time": "2018-01-11T23:03:21.924572Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tensorflow layer for reading wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
