{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T00:20:16.897264Z",
     "start_time": "2018-01-03T00:20:15.272992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import kapre\n",
    "import pandas as pd\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import random\n",
    "import arrow\n",
    "import threading\n",
    "from soph import soph_scaler, center_wave, ex_generator\n",
    "\n",
    "# https://github.com/keunwoochoi/kapre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T00:20:18.004309Z",
     "start_time": "2018-01-03T00:20:17.896724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'displacement': 0, 'l2_reg': 0.0005, 'val_state': ['test'], 'n_mels': 26, 'cnn_blocks': [[[64, 3]], [[64, 3]], [[128, 3]], [[128, 3]]], 'p_transform': 0, 'batch_size': 64, 'shift': 0, 'n_dft': 1024, 'dropout_prob': 0.4, 'vol_range': 0, 'train_state': ['train', 'val'], 'activation': 'elu', 'mel_trainable': False}\n"
     ]
    }
   ],
   "source": [
    "ex_df = pd.read_pickle(\"data/ex_df.pkl\")\n",
    "current_time = arrow.now().to('US/Eastern').format('YYYY-MM-DD-HH-mm')\n",
    "\n",
    "\n",
    "src_args = {\n",
    "    \"dropout_prob\": .4,\n",
    "    \"activation\": \"elu\",\n",
    "    \"batch_size\": 64,\n",
    "    \"l2_reg\": 0.0005,\n",
    "    \"cnn_blocks\": [\n",
    "        [[64, 3]],\n",
    "        [[64, 3]],\n",
    "        [[128, 3]],\n",
    "        [[128, 3]],\n",
    "    ],\n",
    "    \"mel_trainable\": False,\n",
    "    \"train_state\": [\"train\", \"val\"],\n",
    "    \"val_state\": [\"test\"],\n",
    "    \"p_transform\": 0,\n",
    "    \"vol_range\": 0,\n",
    "    \"displacement\": 0,\n",
    "    \"shift\":0,\n",
    "    \"n_mels\": 26,\n",
    "    \"n_dft\": 1024,\n",
    "}\n",
    "num_cat = 12\n",
    "print(src_args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T00:20:21.976032Z",
     "start_time": "2018-01-03T00:20:18.833157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 16000)          0         \n",
      "_________________________________________________________________\n",
      "melspectrogram_1 (Melspectro (None, 26, 32, 1)         1063962   \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 26, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 13, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 6, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 3, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 1, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 1,327,590\n",
      "Trainable params: 262,860\n",
      "Non-trainable params: 1,064,730\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "act = src_args[\"activation\"]\n",
    "kern = (3, 3)\n",
    "drop = src_args[\"dropout_prob\"]\n",
    "cnn_blocks = src_args[\"cnn_blocks\"]\n",
    "\n",
    "input_layers = [\n",
    "    keras.layers.InputLayer(input_shape=(1, 16000)),\n",
    "    kapre.time_frequency.Melspectrogram(\n",
    "        sr=16000,\n",
    "        n_mels=src_args[\"n_mels\"],\n",
    "        return_decibel_melgram=True,\n",
    "        n_dft=src_args[\"n_dft\"]),\n",
    "]\n",
    "\n",
    "cnn_layers = []\n",
    "\n",
    "for block in cnn_blocks:\n",
    "    for layer in block:\n",
    "        cnn_layers.extend([\n",
    "            keras.layers.Conv2D(\n",
    "                layer[0], layer[1], padding=\"same\", activation=act),\n",
    "        ])\n",
    "    cnn_layers.extend([\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(drop),\n",
    "        keras.layers.AvgPool2D(padding='valid'),\n",
    "    ])\n",
    "\n",
    "class_layers = [\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(num_cat, activation=\"softmax\"),\n",
    "]\n",
    "cnn_model = keras.Sequential(input_layers + cnn_layers + class_layers)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-02T22:08:16.476601Z",
     "start_time": "2018-01-02T21:33:14.770641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "936/936 [============================>.] - ETA: 0s - loss: 1.0782 - acc: 0.4816Epoch 00001: val_loss improved from inf to 0.75465, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 97s 104ms/step - loss: 1.0777 - acc: 0.4818 - val_loss: 0.7547 - val_acc: 0.6606\n",
      "Epoch 2/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.7221Epoch 00002: val_loss improved from 0.75465 to 0.48182, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.4802 - acc: 0.7221 - val_loss: 0.4818 - val_acc: 0.7070\n",
      "Epoch 3/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.7745Epoch 00003: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.3902 - acc: 0.7744 - val_loss: 0.8317 - val_acc: 0.8093\n",
      "Epoch 4/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8021Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.0010000000474974513.\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.3497 - acc: 0.8019 - val_loss: 0.6128 - val_acc: 0.8117\n",
      "Epoch 5/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.8341Epoch 00005: val_loss improved from 0.48182 to 0.21450, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2836 - acc: 0.8341 - val_loss: 0.2145 - val_acc: 0.8717\n",
      "Epoch 6/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.8485Epoch 00006: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2692 - acc: 0.8485 - val_loss: 0.2817 - val_acc: 0.8403\n",
      "Epoch 7/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.8513Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.0005000000237487257.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2514 - acc: 0.8513 - val_loss: 0.2730 - val_acc: 0.8403\n",
      "Epoch 8/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.8651Epoch 00008: val_loss improved from 0.21450 to 0.18059, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2234 - acc: 0.8652 - val_loss: 0.1806 - val_acc: 0.9019\n",
      "Epoch 9/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.8722Epoch 00009: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2137 - acc: 0.8721 - val_loss: 0.1856 - val_acc: 0.8825\n",
      "Epoch 10/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.8747Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.0002500000118743628.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.2123 - acc: 0.8746 - val_loss: 0.1904 - val_acc: 0.8751\n",
      "Epoch 11/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.8823Epoch 00011: val_loss improved from 0.18059 to 0.15798, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1949 - acc: 0.8823 - val_loss: 0.1580 - val_acc: 0.9060\n",
      "Epoch 12/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.8862Epoch 00012: val_loss improved from 0.15798 to 0.15533, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1902 - acc: 0.8863 - val_loss: 0.1553 - val_acc: 0.9056\n",
      "Epoch 13/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.8881Epoch 00013: val_loss improved from 0.15533 to 0.15244, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1854 - acc: 0.8881 - val_loss: 0.1524 - val_acc: 0.9173\n",
      "Epoch 14/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.8917Epoch 00014: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1810 - acc: 0.8916 - val_loss: 0.1566 - val_acc: 0.8999\n",
      "Epoch 15/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.8877Epoch 00015: val_loss improved from 0.15244 to 0.15071, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1816 - acc: 0.8877 - val_loss: 0.1507 - val_acc: 0.9022\n",
      "Epoch 16/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.8926Epoch 00016: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1776 - acc: 0.8927 - val_loss: 0.1574 - val_acc: 0.9085\n",
      "Epoch 17/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.8933Epoch 00017: val_loss improved from 0.15071 to 0.14968, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1741 - acc: 0.8934 - val_loss: 0.1497 - val_acc: 0.9156\n",
      "Epoch 18/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.8980Epoch 00018: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1709 - acc: 0.8979 - val_loss: 0.1512 - val_acc: 0.9090\n",
      "Epoch 19/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.8967Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0001250000059371814.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1688 - acc: 0.8967 - val_loss: 0.1573 - val_acc: 0.9064\n",
      "Epoch 20/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.8961Epoch 00020: val_loss improved from 0.14968 to 0.14434, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1657 - acc: 0.8962 - val_loss: 0.1443 - val_acc: 0.9154\n",
      "Epoch 21/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.8982Epoch 00021: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1637 - acc: 0.8983 - val_loss: 0.1451 - val_acc: 0.9187\n",
      "Epoch 22/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9025Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 6.25000029685907e-05.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1565 - acc: 0.9026 - val_loss: 0.1454 - val_acc: 0.9214\n",
      "Epoch 23/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9011Epoch 00023: val_loss improved from 0.14434 to 0.14115, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1551 - acc: 0.9011 - val_loss: 0.1412 - val_acc: 0.9190\n",
      "Epoch 24/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9029Epoch 00024: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1547 - acc: 0.9029 - val_loss: 0.1438 - val_acc: 0.9157\n",
      "Epoch 25/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9040Epoch 00025: val_loss improved from 0.14115 to 0.14082, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1561 - acc: 0.9040 - val_loss: 0.1408 - val_acc: 0.9201\n",
      "Epoch 26/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9071Epoch 00026: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1537 - acc: 0.9070 - val_loss: 0.1445 - val_acc: 0.9239\n",
      "Epoch 27/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9024Epoch 00027: val_loss improved from 0.14082 to 0.14042, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1553 - acc: 0.9024 - val_loss: 0.1404 - val_acc: 0.9247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9047Epoch 00028: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1528 - acc: 0.9047 - val_loss: 0.1428 - val_acc: 0.9240\n",
      "Epoch 29/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9057Epoch 00029: val_loss did not improve\n",
      "\n",
      "Epoch 00029: reducing learning rate to 3.125000148429535e-05.\n",
      "937/936 [==============================] - 38s 41ms/step - loss: 0.1510 - acc: 0.9056 - val_loss: 0.1422 - val_acc: 0.9214\n",
      "Epoch 30/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9075Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 1.5625000742147677e-05.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1527 - acc: 0.9076 - val_loss: 0.1414 - val_acc: 0.9242\n",
      "Epoch 31/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9069Epoch 00031: val_loss improved from 0.14042 to 0.13958, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1481 - acc: 0.9069 - val_loss: 0.1396 - val_acc: 0.9228\n",
      "Epoch 32/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9066Epoch 00032: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1507 - acc: 0.9066 - val_loss: 0.1412 - val_acc: 0.9215\n",
      "Epoch 33/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9068Epoch 00033: val_loss improved from 0.13958 to 0.13953, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "\n",
      "Epoch 00033: reducing learning rate to 7.812500371073838e-06.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1501 - acc: 0.9068 - val_loss: 0.1395 - val_acc: 0.9240\n",
      "Epoch 34/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9061Epoch 00034: val_loss improved from 0.13953 to 0.13949, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "\n",
      "Epoch 00034: reducing learning rate to 3.906250185536919e-06.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1484 - acc: 0.9061 - val_loss: 0.1395 - val_acc: 0.9222\n",
      "Epoch 35/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9064Epoch 00035: val_loss did not improve\n",
      "\n",
      "Epoch 00035: reducing learning rate to 1.9531250927684596e-06.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1505 - acc: 0.9064 - val_loss: 0.1403 - val_acc: 0.9223\n",
      "Epoch 36/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9065Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 9.765625463842298e-07.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1511 - acc: 0.9065 - val_loss: 0.1400 - val_acc: 0.9240\n",
      "Epoch 37/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9081Epoch 00037: val_loss did not improve\n",
      "\n",
      "Epoch 00037: reducing learning rate to 4.882812731921149e-07.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1482 - acc: 0.9082 - val_loss: 0.1401 - val_acc: 0.9223\n",
      "Epoch 38/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9071Epoch 00038: val_loss did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 2.4414063659605745e-07.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1493 - acc: 0.9071 - val_loss: 0.1400 - val_acc: 0.9232\n",
      "Epoch 39/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9064Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00039: reducing learning rate to 1.2207031829802872e-07.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1503 - acc: 0.9063 - val_loss: 0.1402 - val_acc: 0.9233\n",
      "Epoch 40/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9060Epoch 00040: val_loss did not improve\n",
      "\n",
      "Epoch 00040: reducing learning rate to 6.103515914901436e-08.\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1500 - acc: 0.9060 - val_loss: 0.1401 - val_acc: 0.9228\n",
      "Epoch 41/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9063Epoch 00041: val_loss did not improve\n",
      "\n",
      "Epoch 00041: reducing learning rate to 3.051757957450718e-08.\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1477 - acc: 0.9064 - val_loss: 0.1395 - val_acc: 0.9237\n",
      "Epoch 42/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9061Epoch 00042: val_loss improved from 0.13949 to 0.13948, saving model to logs/cnn/2018-01-02-16-32/model-checkpoint.hdf5\n",
      "\n",
      "Epoch 00042: reducing learning rate to 1.525878978725359e-08.\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1473 - acc: 0.9061 - val_loss: 0.1395 - val_acc: 0.9232\n",
      "Epoch 43/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9081Epoch 00043: val_loss did not improve\n",
      "\n",
      "Epoch 00043: reducing learning rate to 1e-08.\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1466 - acc: 0.9081 - val_loss: 0.1400 - val_acc: 0.9237\n",
      "Epoch 44/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9080Epoch 00044: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1484 - acc: 0.9080 - val_loss: 0.1400 - val_acc: 0.9236\n",
      "Epoch 45/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9091Epoch 00045: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1474 - acc: 0.9091 - val_loss: 0.1400 - val_acc: 0.9225\n",
      "Epoch 46/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9069Epoch 00046: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1500 - acc: 0.9069 - val_loss: 0.1398 - val_acc: 0.9240\n",
      "Epoch 47/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9076Epoch 00047: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1543 - acc: 0.9077 - val_loss: 0.1407 - val_acc: 0.9230\n",
      "Epoch 48/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9082Epoch 00048: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1482 - acc: 0.9080 - val_loss: 0.1401 - val_acc: 0.9235\n",
      "Epoch 49/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9077Epoch 00049: val_loss did not improve\n",
      "937/936 [==============================] - 39s 41ms/step - loss: 0.1508 - acc: 0.9076 - val_loss: 0.1400 - val_acc: 0.9240\n",
      "Epoch 50/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9074Epoch 00050: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1479 - acc: 0.9074 - val_loss: 0.1402 - val_acc: 0.9236\n",
      "Epoch 51/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9063Epoch 00051: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1491 - acc: 0.9063 - val_loss: 0.1407 - val_acc: 0.9230\n",
      "Epoch 52/200\n",
      "935/936 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9069Epoch 00052: val_loss did not improve\n",
      "937/936 [==============================] - 39s 42ms/step - loss: 0.1468 - acc: 0.9069 - val_loss: 0.1396 - val_acc: 0.9235\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f019d42bbe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_base = \"logs/cnn/{}/\".format(current_time)\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir= log_base+'tb',\n",
    "        batch_size=src_args[\"batch_size\"],\n",
    "        histogram_freq=0,\n",
    "        write_grads=False, \n",
    "        write_images=True\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=log_base+'model-checkpoint.hdf5',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='auto',\n",
    "        period=1),\n",
    "    keras.callbacks.CSVLogger(log_base+'training.log'),\n",
    "    keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.5, patience=1, verbose=1, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "def launchTensorBoard():\n",
    "    import os\n",
    "    os.system('pkill tensorboard')\n",
    "    os.system('tensorboard --logdir=' + log_base+'tb')\n",
    "    return\n",
    "\n",
    "t = threading.Thread(target=launchTensorBoard, args=([]))\n",
    "t.start()\n",
    "\n",
    "val_data = next(ex_generator(\n",
    "        batch_size=sum(ex_df.state.isin(src_args[\"val_state\"])),\n",
    "        shuffle=False,\n",
    "        state=src_args[\"val_state\"],\n",
    "        vol_range=0,\n",
    "        displacement=0,\n",
    "        p_transform=0))\n",
    "\n",
    "cnn_model.fit_generator(\n",
    "    generator=ex_generator(\n",
    "        batch_size=src_args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        state=src_args[\"train_state\"],\n",
    "        shift=src_args[\"shift\"],\n",
    "        vol_range=src_args[\"vol_range\"],\n",
    "        displacement=src_args[\"displacement\"],\n",
    "        p_transform=src_args[\"p_transform\"]),\n",
    "    steps_per_epoch=sum(ex_df.state.isin(src_args[\"train_state\"])) / src_args[\"batch_size\"],\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    max_queue_size=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
